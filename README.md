# llama-bot

تشغيل :contentReference[oaicite:1]{index=1} (8B) + بوت :contentReference[oaicite:2]{index=2} عبر :contentReference[oaicite:3]{index=3} / أي سيرفر Docker‑ready

## ⚙️ خطوات الإعداد

1. أنشئ مجلد `models/` داخل المشروع وضع فيه الملف:
   `LLaMA-3.1-8B-Instruct-Q4_0.gguf`  
   (لا ترفعه على GitHub — لأنه كبير الحجم).

2. اربط المشروع مع Render → Web Service → نوع Docker → Free Tier.

3. بعد البناء (build) سيظهر لك رابط مثل:
